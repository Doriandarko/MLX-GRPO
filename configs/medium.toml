# Medium training config - Sweet spot for seeing actual results!
# ~20-30 minutes, 400-800 samples, good chance of learning format

model_name = "utils/mlx_model"    # Your converted model
output_dir = "outputs/medium-run"
run_name   = "qwen-1.5b-medium"

# training
learning_rate = 1e-6
num_epochs = 1
batch_size = 1
gradient_accumulation_steps = 2
max_train_samples = 200           # 200 examples Ã— 4 gens = 800 samples!
warmup_ratio = 0.1
max_grad_norm = 0.1
logging_steps = 1
save_steps = 50                   # Save every 50 steps

# sampling / rollout (balanced)
num_generations = 4               # 4 generations per prompt (good balance)
max_new_tokens = 128              # 128 tokens (enough for reasoning)
max_prompt_length = 512
max_completion_length = 512
temperature = 0.7
clip_eps = 0.2
kl_coeff = 0.0

# eval & logging
eval_steps = 25
eval_samples = 50
eval_every_updates = 25
eval_subset_size = 50
eval_max_new_tokens = 128
log_jsonl = true

# system
seed = 42
use_compile = true

