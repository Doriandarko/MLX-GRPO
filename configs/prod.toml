# Production run settings - Full DeepSeek-inspired training
# WARNING: This trains on 1000+ examples and takes 4-8+ HOURS!
# Use medium.toml for faster results

model_name = "utils/mlx_model"    # Your converted model (change if different)
output_dir = "outputs/production-run"
run_name   = "qwen-1.5b-production"

# training
learning_rate = 1e-6
num_epochs = 1
batch_size = 1
gradient_accumulation_steps = 4
max_train_samples = 1000          # Limit to 1000 (full = 7473 takes forever!)
warmup_ratio = 0.1
max_grad_norm = 0.1
logging_steps = 1
save_steps = 100

# sampling / rollout
num_generations = 64
max_new_tokens = 512
max_prompt_length = 512
max_completion_length = 1024
temperature = 0.7
clip_eps = 0.2
kl_coeff = 0.0

# eval & logging
eval_steps = 50
eval_samples = 200
eval_every_updates = 25
eval_subset_size = 200
eval_max_new_tokens = 512
log_jsonl = true

# system
seed = 0
use_compile = true

