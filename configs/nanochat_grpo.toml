# Nanochat GRPO Training Configuration
# Full training run to improve nanochat with GRPO

model_name = "models/nanochat-mlx"
output_dir = "outputs/nanochat-grpo"
run_name   = "nanochat-grpo-v1"

# Training parameters
learning_rate = 5e-6              # Conservative LR for small model
num_epochs = 3                    # Multiple passes through data
batch_size = 1                    # Keep memory usage low
gradient_accumulation_steps = 4   # Effective batch size of 4
max_train_samples = 500           # Train on 500 examples to start
warmup_ratio = 0.05               # 5% warmup
max_grad_norm = 1.0               # Gradient clipping
logging_steps = 1                 # Log every step

# GRPO sampling parameters
num_generations = 4               # Generate 4 responses per prompt
max_new_tokens = 64               # Allow longer reasoning
temperature = 0.8                 # Balanced exploration/exploitation
clip_eps = 0.2                    # PPO-style clipping
kl_coeff = 0.01                   # Small KL penalty to prevent drift

# Evaluation
eval_steps = 25                   # Evaluate every 25 steps
eval_samples = 20                 # Use 20 samples for eval
eval_every_updates = 25
eval_subset_size = 20
eval_max_new_tokens = 64
save_steps = 50                   # Save checkpoint every 50 steps
log_jsonl = true

# System
seed = 42
use_compile = false               # Disable for stability

# Note: With these settings:
# - 500 samples / (batch_size=1 * grad_accum=4) = 125 updates per epoch
# - 125 updates * 3 epochs = 375 total updates
# - Each sample generates 4 responses = 2000 generations total
# - Should take ~30-60 minutes depending on hardware


